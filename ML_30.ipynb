{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://training.dwit.edu.np/frontend/images/computer-training-institute.png'>\n",
    "<h1>Data Science and Machine learning in Python</h1>\n",
    "<h3>Instructor: <a href='https://www.kaggle.com/atishadhikari'> Atish Adhikari</a></h3>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis with RNN - LSTM\n",
    "#### Dataset: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences\n",
    "#### Embedding Vectors: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"datasets/imdb_labelled.txt\", sep=\"\\t\", header=None, names=[\"sentence\", \"sentiment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  sentiment\n",
       "0  A very, very, very slow-moving, aimless movie ...          0\n",
       "1  Not sure who was more lost - the flat characte...          0\n",
       "2  Attempting artiness with black & white and cle...          0\n",
       "3       Very little music or anything to speak of.            0\n",
       "4  The best scene in the movie was when Gerardo i...          1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "punctuations = [\"!\", \"?\", \",\", \"'\", '\"', \";\", \":\", \".\"]\n",
    "\n",
    "\n",
    "for comment in data[\"sentence\"].values:\n",
    "    comment = comment.lower()\n",
    "    sentence = nltk.tokenize.sent_tokenize(comment)\n",
    "    for sen in sentence:\n",
    "        words = nltk.tokenize.word_tokenize(sen)\n",
    "        #remove punctuations\n",
    "        filtered_words = [w.lower() for w in words if w not in punctuations]\n",
    "            \n",
    "    X.append(filtered_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.Word2Vec(sentences=X, size=100, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2vec.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atish\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('of', 0.9997719526290894),\n",
       " ('to', 0.9997655153274536),\n",
       " ('and', 0.9997645020484924),\n",
       " ('i', 0.9997337460517883),\n",
       " ('it', 0.9997329711914062),\n",
       " ('more', 0.9997294545173645),\n",
       " ('the', 0.999719500541687),\n",
       " ('that', 0.9997149705886841),\n",
       " ('is', 0.9997129440307617),\n",
       " ('a', 0.9997122287750244)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar(\"bad\")\n",
    "#not well formed because of less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert to wordtovec as gensim only has word2vec-modal\n",
    "gensim.scripts.glove2word2vec.glove2word2vec(glove_input_file=\"large_datasets/glove.6B.200d.txt\", word2vec_output_file=\"large_datasets/word2vec.6B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load word2vev modal\n",
    "word2vec_modal = gensim.models.KeyedVectors.load_word2vec_format(\"large_datasets/word2vec.6B.200d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bhutan', 0.7069970965385437),\n",
       " ('kathmandu', 0.702731728553772),\n",
       " ('nepali', 0.7026580572128296),\n",
       " ('nepalese', 0.6685097813606262),\n",
       " ('bangladesh', 0.6385263204574585),\n",
       " ('maoist', 0.6186525225639343),\n",
       " ('cambodia', 0.6165409088134766),\n",
       " ('india', 0.6137661933898926),\n",
       " ('laos', 0.6054975390434265),\n",
       " ('myanmar', 0.5960168838500977)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_modal.most_similar(\"nepal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6978678703308105),\n",
       " ('princess', 0.6081745028495789),\n",
       " ('monarch', 0.5889754891395569),\n",
       " ('throne', 0.5775108933448792),\n",
       " ('prince', 0.5750998854637146),\n",
       " ('elizabeth', 0.546359658241272),\n",
       " ('daughter', 0.5399125814437866),\n",
       " ('kingdom', 0.5318052768707275),\n",
       " ('mother', 0.5168544054031372),\n",
       " ('crown', 0.5164472460746765)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_modal.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"sentence\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Represent every word by unique numerical token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preserve mapping of word -> token in a variable for later\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \n",
      "[3, 28, 28, 28, 287, 407, 1216, 12, 37, 3, 1217, 1218, 408, 143]\n"
     ]
    }
   ],
   "source": [
    "#orginal and tokenized form\n",
    "print(X[0])\n",
    "print(X_tokenized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieving token for a word\n",
    "word_index[\"very\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_length = [len(x) for x in X_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
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
     ]
    }
   ],
   "source": [
    "#checking length of each comments in imdb\n",
    "for i in comment_length:\n",
    "    print(i, end=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad/Threshold all comments to a fixed size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thresholding max comment length to 35\n",
    "max_len = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the comments with less words, thresholding comments with many words\n",
    "X_padded = pad_sequences(X_tokenized, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A very, very, very slow-moving, aimless movie about a distressed, drifting young man.  \n",
      "\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    3   28   28   28  287  407 1216\n",
      "   12   37    3 1217 1218  408  143]\n"
     ]
    }
   ],
   "source": [
    "print(X[0])\n",
    "print()\n",
    "print(X_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get word -> vector mapping from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Convert Glove file to embedding matrix \n",
    "#Get word -> vector mapping from file\n",
    "embeddings_index = {}\n",
    "f = open('large_datasets/glove.6B.200d.txt', encoding='UTF-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.1827e-01,  1.9440e-02,  8.9215e-03, -1.7083e-01, -1.5074e-01,\n",
       "        4.4691e-02, -8.4464e-01,  3.3799e-01,  7.0777e-01, -2.4093e-01,\n",
       "        4.0375e-01,  5.4616e-01, -1.8541e-01, -6.6753e-02,  3.8632e-01,\n",
       "        5.3156e-01, -1.2843e-01,  3.7425e-01,  3.3350e-01, -5.2774e-01,\n",
       "        2.4610e-01,  2.5388e+00, -8.1737e-02, -2.1850e-02,  2.8038e-01,\n",
       "       -2.2604e-01,  7.2170e-02,  5.3669e-01,  9.0519e-02, -3.3273e-01,\n",
       "        4.3203e-02,  3.1065e-02, -5.1937e-02,  3.0823e-01,  2.5642e-01,\n",
       "       -3.4948e-01, -1.0253e+00, -5.8867e-01, -4.9620e-01,  3.8668e-01,\n",
       "       -5.1096e-01, -7.8659e-02, -3.3842e-01,  5.1494e-01, -3.3330e-01,\n",
       "       -5.6195e-01,  7.7807e-01,  2.1362e-01, -1.5577e-02, -1.7699e-01,\n",
       "       -2.4518e-01, -2.1210e-01, -3.8734e-01,  4.8660e-01,  4.6362e-01,\n",
       "       -3.1572e-01,  1.6519e-01, -2.6461e-01, -4.1045e-01, -3.9097e-01,\n",
       "        3.2260e-01,  1.5388e-01, -2.8839e-01,  3.3369e-01, -2.6836e-01,\n",
       "        2.4708e-01,  1.8982e-01,  4.2192e-01, -6.7222e-02,  3.2684e-01,\n",
       "        5.6248e-01, -3.3984e-01, -3.6503e-01,  7.4364e-01, -4.7445e-01,\n",
       "       -2.9718e-01, -8.6057e-01, -4.3411e-01, -5.0071e-01, -3.3460e-02,\n",
       "       -1.5457e-01,  4.1711e-02, -1.8984e-01,  2.9394e-01,  7.5474e-01,\n",
       "        3.3217e-01,  1.8305e-01, -1.7194e-01,  9.3167e-01, -1.1665e+00,\n",
       "        4.9196e-02, -2.4545e-01,  5.4277e-02, -4.0228e-01, -1.2895e-01,\n",
       "       -1.4051e-01,  4.0895e-01,  1.4405e-01,  3.4732e-02,  2.8172e-01,\n",
       "        6.4137e-01,  9.4091e-02,  1.8892e-01, -1.1083e-01,  3.1373e-01,\n",
       "        6.5711e-02, -1.3134e-01,  8.9254e-01, -6.0287e-01,  2.7107e-01,\n",
       "       -1.6172e-01, -8.3740e-02,  2.5290e-01, -3.8541e-01,  1.0659e-01,\n",
       "       -5.8669e-02, -2.6934e-02, -3.0704e-01, -2.7050e-01, -1.3265e-01,\n",
       "        7.7925e-01,  1.7711e-01,  2.0641e-01, -3.9107e-01, -5.1651e-01,\n",
       "       -1.5932e-01,  3.9448e-01,  2.9664e-01,  2.0355e-01, -1.9777e-01,\n",
       "        9.9442e-02, -1.0178e-01,  2.9832e-01, -1.0684e-01,  2.4005e-01,\n",
       "       -2.1897e-02, -4.3692e-01, -1.2476e-02, -1.9230e-01, -3.3930e-01,\n",
       "       -3.7047e-01, -7.8218e-03,  3.3615e-02, -4.8621e-03,  1.8096e+00,\n",
       "        1.8998e-01,  5.6783e-02, -7.1863e-01,  2.0231e-01,  3.1358e-01,\n",
       "        1.3790e-01,  4.4480e-01, -7.4320e-02,  1.7495e-01,  5.6862e-01,\n",
       "       -4.9797e-01, -1.3720e-01,  2.0976e-01, -4.4892e-02, -1.0549e-01,\n",
       "        2.9161e-01, -1.7454e-01, -3.8699e-01,  3.2588e-01, -2.0592e-01,\n",
       "       -2.2608e-01, -6.9086e-02,  4.6330e-01, -3.7988e-01,  3.2099e-01,\n",
       "       -3.1524e-01, -3.0861e-01,  1.4385e-01,  1.3070e-01,  2.9129e-02,\n",
       "       -1.0769e-01, -3.3859e-01, -4.8886e-01,  1.7311e-01,  2.9914e-01,\n",
       "        1.5208e+00, -9.1614e-02, -2.9671e-01,  3.2993e-01,  5.3068e-02,\n",
       "        2.5903e-01,  3.3094e-01,  1.8074e-03,  1.7075e-02,  3.5243e-01,\n",
       "       -6.4061e-02, -1.1377e-01, -1.4051e-01, -1.4444e-01,  5.2523e-01,\n",
       "        1.7551e-01,  1.3426e-01,  2.6377e-01, -4.4168e-02, -3.6501e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get word -> vector mapping from file for sample 'very'\n",
    "embeddings_index[\"very\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a matrix with embedding vector at corresponding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_index from earlier is used here....\n",
    "\n",
    "#Uses 0 for unknown words and padded 0s, so size is vocab_size + 1\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 200))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Sequential()\n",
    "\n",
    "#pass embedding matrix for embedding layer, make it non-trainable\n",
    "rnn.add(Embedding(input_dim=(len(word_index) + 1), output_dim=200, input_length=max_len, \n",
    "                   weights=[embedding_matrix], trainable=False ))\n",
    "        \n",
    "rnn.add(LSTM(units=100))\n",
    "rnn.add(Dense(units=\"50\", activation=\"relu\"))\n",
    "rnn.add(Dense(units=\"1\", activation=\"sigmoid\"))\n",
    "\n",
    "rnn.compile(loss=\"binary_crossentropy\", metrics=[\"acc\"], optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"sentiment\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "22/22 [==============================] - 2s 70ms/step - loss: 0.1462 - acc: 0.9510 - val_loss: 0.0620 - val_acc: 0.9867\n",
      "Epoch 2/10\n",
      "22/22 [==============================] - 1s 41ms/step - loss: 0.0968 - acc: 0.9658 - val_loss: 0.0640 - val_acc: 0.9733\n",
      "Epoch 3/10\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0685 - acc: 0.9837 - val_loss: 0.0901 - val_acc: 0.9467\n",
      "Epoch 4/10\n",
      "22/22 [==============================] - 1s 49ms/step - loss: 0.0500 - acc: 0.9881 - val_loss: 0.0854 - val_acc: 0.9467\n",
      "Epoch 5/10\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0921 - acc: 0.9688 - val_loss: 0.1413 - val_acc: 0.9067\n",
      "Epoch 6/10\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0681 - acc: 0.9792 - val_loss: 0.0527 - val_acc: 0.9867\n",
      "Epoch 7/10\n",
      "22/22 [==============================] - 1s 45ms/step - loss: 0.0428 - acc: 0.9911 - val_loss: 0.2082 - val_acc: 0.8933\n",
      "Epoch 8/10\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0349 - acc: 0.9926 - val_loss: 0.0635 - val_acc: 0.9733\n",
      "Epoch 9/10\n",
      "22/22 [==============================] - 1s 42ms/step - loss: 0.0698 - acc: 0.9777 - val_loss: 0.0957 - val_acc: 0.9600\n",
      "Epoch 10/10\n",
      "22/22 [==============================] - 1s 44ms/step - loss: 0.0307 - acc: 0.9955 - val_loss: 0.0567 - val_acc: 0.9733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d65e103a58>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.fit(X_padded, y, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Very good movie\",\n",
    "    \"worst movie ever\",\n",
    "    \"movie was boring, but did not like acting\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens = tokenizer.texts_to_sequences(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_paded = pad_sequences(sent_tokens, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,  28,  32,  12],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0, 176,  12,  65],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  12,  11, 203,  19, 173,  25,  40,  43]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_paded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict sentiment ( 1 => Good comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.994603  ],\n",
       "       [0.01458338],\n",
       "       [0.00985349]], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.predict(sentences_paded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
